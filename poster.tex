%
\documentclass[final]{beamer} 
  \mode<presentation> {  
    \usetheme{Yale} 
  }

\definecolor{beamer@blendedblue}{HTML}{BD5319}
\usepackage{verbatim,bm}
\DeclareMathOperator*{\supp}{supp}
\DeclareMathOperator*{\diam}{diam}
\usepackage{tikz,fp}
\usetikzlibrary{calc,patterns}
  \usepackage[english]{babel}
%
  \usepackage{amsmath,amsthm, amssymb, latexsym}
  \usepackage[ruled,linesnumbered,vlined]{algorithm2e}
  \usepackage{bm}
  \usepackage{tcolorbox}
\usepackage{tabularx}
\usepackage{array}
\usepackage{colortbl}
\usepackage{cleveref}
\tcbuselibrary{skins}


\definecolor{YaleLightBlue}{HTML}{63AAFF}
\definecolor{YaleCoreBlue}{HTML}{00356B}
\definecolor{YaleDarkBlue}{HTML}{286DC0}
\definecolor{YaleLighterGrey}{HTML}{DDDDDD}
\definecolor{YaleLightGrey}{HTML}{978D85}
\definecolor{YaleCoreGrey}{HTML}{4A4A4A}
\definecolor{YaleDarkGrey}{HTML}{222222}
\definecolor{YaleCoreOrange}{HTML}{BD5319}
\definecolor{YaleCoreGreen}{HTML}{5F712D}
\definecolor{YaleGreen}{HTML}{5F712D}
\definecolor{DarkRed}{HTML}{B8433B}
\setbeamertemplate{caption}{\raggedright\insertcaption\par}
\setbeamercolor{caption}{fg=YaleCoreGreen}
\setlength\abovecaptionskip{-20pt}

\tcbset{tab2/.style={enhanced,fonttitle=\bfseries,fontupper=\normalsize\sffamily,
colback=YaleLightBlue!10!white,colframe=YaleCoreBlue,colbacktitle=YaleCoreBlue,
coltitle=black,center title}}




\newcommand{\Acc}{\text{Acc.}}
\newcommand{\Dis}{\text{G}_{\text{Discr.}}}
\newcommand{\LDis}{\text{L}_{\text{Discr.}}}
\newcommand{\Knn}{\text{kNN-Pred}}
\newcommand{\spaceClass}{\mathcal{H}}
\newcommand{\optimal}{h^*}
\newcommand{\optimalfair}{h^{*}_{\textup{fair}}}
\newcommand{\kr}{Kre\u{\i}n\xspace}

\DeclareMathOperator*{\essinf}{ess\,inf}
\DeclareMathOperator*{\esssup}{ess\,sup}
\DeclareMathOperator{\hel}{hel}
\DeclareMathOperator{\pr}{Pr}

\DeclareMathOperator{\sech}{sech}
\DeclareMathOperator{\csch}{csch}
\DeclareMathOperator{\mil}{mil}
\DeclareMathOperator{\sign}{sign}

\newcommand{\kl}[2]{D_{\mathrm{KL}}( #1 \parallel #2 )}
\newcommand{\js}[2]{D_{\mathrm{JS}}( #1 \parallel #2 )}
\newcommand{\wjs}[3]{D_{\mathrm{GJS}}^{#1}( #2 \parallel #3 )}
%\newcommand{\mil}[2]{D_{\mathrm{MIL}}( #1 \parallel #2 )}
\newcommand{\td}[2]{ \Delta ( #1 \parallel #2 )}


  %
  \usepackage{wrapfig}
  \usefonttheme[onlymath]{serif}
  \usepackage{graphicx}
  \boldmath
\usepackage[size=A0,scale=1.4,orientation=landskape]{beamerposter} 
\title{Locality-Sensitive Hashing for $f$-Divergences and \kr Kernels: 
	Mutual 
	Information Loss and Beyond}
\author{Lin Chen\inst{1,2}, 
	Hossein Esfandiari\inst{1},
	Thomas Fu\inst{1}, and
	Vahab Mirrokni\inst{1}
	 \vspace{10pt}}
\institute{{  \inst{1}Google Research, \inst{2}Yale University }\vspace{10pt}}
%

\newlength{\columnheight}
%
\setlength{\columnheight}{105cm}
%

\usepackage{forloop}
\newcommand{\defcal}[1]{\expandafter\newcommand\csname c#1\endcsname{{\mathcal{#1}}}}
\newcommand{\defbb}[1]{\expandafter\newcommand\csname b#1\endcsname{{\mathbb{#1}}}}
\newcounter{calBbCounter}
\forLoop{1}{26}{calBbCounter}{
	\edef\letter{\Alph{calBbCounter}}
	\expandafter\defcal\letter
	\expandafter\defbb\letter
}


\usepackage[absolute,overlay]{textpos}
%
\renewcommand{\vec}[1]{\mathbf{#1}}

\usepackage{nicefrac}       %
 
%
\PassOptionsToPackage{table,pdftex,dvipsnames}{xcolor}
\usepackage{array,booktabs,tabularx}
\usepackage{multirow}
\usepackage[numbers]{natbib}
\usepackage{subfigure}
\newcommand{\matroid}{\mathcal{I}}
\DeclareMathOperator{\round}{round}
\newcommand{\one}{\mathbf{1}}
\newcommand{\constraint}{\mathcal{K}}
\newcommand{\ie}{\emph{i.e.}\xspace}
\newcommand{\AlgMeta}{\texttt{Meta-Frank-Wolfe}\xspace}
\newcommand{\AlgVR}{\texttt{VR-Frank-Wolfe}\xspace}
\newcommand{\Alg}{\texttt{Mono-Frank-Wolfe}\xspace}
\newcommand{\AlgB}{\texttt{Bandit-Frank-Wolfe}\xspace}
\newcommand{\AlgD}{\texttt{Responsive-Frank-Wolfe}\xspace}
\newcommand{\OCSM}{\texttt{OCSM}\xspace}
\newcommand{\BCSM}{\texttt{BCSM}\xspace}
\newcommand{\RBSM}{\texttt{RBSM}\xspace}
\newcommand{\BSM}{\texttt{BSM}\xspace}

%
\newcommand{\ground}{V}
\newcommand{\AlgHybrid}{{\textsc{\textsc{Batch-Sieve-Streaming}\texttt{++}}}\xspace}
\newcommand{\AlgParallelSampling}{{\textsc{Parallel-Threshold-Sampling}}\xspace}
\newcommand{\AlgDistributedReducedMean}{{\textsc{Distributed-Reduced-Mean}}\xspace}
\newcommand{\AlgUniform}{\cU_{\textsc{Distributed}}\xspace}
\newcommand{\AlgThresholdSampling}{{\textsc{Threshold-Sampling}}\xspace}
\newcommand{\AlgSieveStreamingPlus}{{\textsc{SieveStreaming}\texttt{++}}\xspace}
\newcommand{\AlgSieveStreaming}{{\textsc{SieveStreaming}}\xspace}
\newcommand{\AlgStreamingPreemption}{{\textsc{PreemptionStreaming}}\xspace}
\newcommand{\AlgOne}{{\textsc{Sample-One-Streaming}}\xspace}
\newcommand{\Opt}{\texttt{OPT}\xspace}
\newcommand{\LB}{\texttt{LB}\xspace}
\newcommand{\ind}{\mathds{1}}
\newcommand{\threOne}{{\texttt{Threshold}}}
\newcommand{\threTwo}{{\texttt{Threshold}_2}}
\newcommand{\memory}{\texttt{B}}
\newcommand{\ratio}{\texttt{C}}
\newcommand{\buffer}{\cB}
\newcommand{\AlgMulti}{{\textsc{MultiSource-Streaming}}\xspace}
\newcommand{\AlgSampling}{{\textsc{Threshold-Sampling}}\xspace}
\newcommand{\Prob}[2]{\Pr_{#1}\parens*{#2}}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
%
\usepackage{setspace}
%
\newlength{\sepwid}
\newlength{\onecolwid}
\newlength{\twocolwid}
\newlength{\threecolwid}
%
%
\setlength{\paperwidth}{48in} %
\setlength{\paperheight}{36in} %
\setlength{\sepwid}{0.001\paperwidth} %
\setlength{\onecolwid}{0.32\paperwidth} %
\setlength{\twocolwid}{0.464\paperwidth} %
\setlength{\threecolwid}{0.708\paperwidth} %
\setlength{\topmargin}{-0.9in} %
%
\usepackage{bm}
\usepackage{graphicx}  %
\usepackage{booktabs} %
\usepackage{rotating}
\begin{document}
	
	%
	%
	
	%
	%
	
	\begin{frame}[t] %
	\begin{columns}[t] %
		%
	\begin{column}{\onecolwid} %
			\vspace{-40pt}
				
			\begin{block}{1. Locality-Sensitive Hashing (LSH)}	
				$ \mathcal{M} $: the universal set of items (the 
				database), endowed with 
				a distance function $ D $.	
				
				\structure{\textbf{Intuition of LSH:}} 
				For any $ p $ and $ q $ in $ \mathcal{M} $,
				\begin{itemize}
					\item If they are \structure{close}, they are more likely 
					to 
					have the 
					\structure{same} hash value;
					\item If they are \structure{far apart}, 
					they are more likely to have \structure{different} hash 
					values. 
				\end{itemize}
				 
				
				 \structure{\textbf{$ (r_1,r_2,p_1,p_2) 
						$-sensitive LSH:}} Let $ \mathcal{H} = \{ 
				h:\mathcal{M}\to U \} $ be a 
				%	(potentially infinite) 
				family of hash functions, where $ U $ is the set of possible 
				hash values. 
				Assume that there is a distribution $ h\sim 
				\mathcal{H} $ over the family of 
				functions. This family $ \mathcal{H} $ is called $ (r_1, r_2, 
				p_1,p_2) 
				$-sensitive  ($ r_1<r_2 $ and $ p_1>p_2 $) for $ D $, if for $ 
				\forall p, q 
				\in \mathcal{M} $ the following statements hold:
				\begin{itemize}
					\item If $ D(p, q) \le r_1 $, then $ \pr_{h\sim 
						\mathcal{H}}[h(p)=h(q)]\ge p_1 $; 
					\item If $ D(p, q) > r_2 $, then $ \pr_{h\sim 
						\mathcal{H}}[h(p)=h(q)]\le p_2 $.
				\end{itemize}
				

%				 We would like to note that the gap between the high 
%				probability 
%				$ p_1 $ and $ 
%				p_2 $ can be amplified by constructing a compound hash function 
%				that 
%				concatenates multiple functions from an LSH family. For 
%				example, one can 
%				construct $ g:\mathcal{M}\to U^K $ such that $ g(p)\triangleq 
%				(h_1(p), \dots, 
%				h_K(p)) $ for $ \forall p\in \mathcal{M} $, where $ h_1,\dots, 
%				h_K $ are chosen 
%				from the LSH family $ \mathcal{H} $. This conjunctive 
%				construction reduces the 
%				amount of items in one bucket. To improve the recall, an 
%				additional disjunction 
%				is introduced. To be precise, if $ g_1,\dots, g_L $ are $ L $ 
%				such compound 
%				hash functions, we search all of the buckets $ g_1(p), \dots, 
%				g_L(p) $ in order 
%				to 
%				find the nearest neighbors of $ p $.	 
		%
		\end{block}
			
		
	\begin{block}{2. $ f $-Divergence}
%			Let $ P $ and $ Q $ be two probability measures associated with a 
%			common sample 
%			space $ \Omega $. We write $ P\ll Q$ if $ P $ is absolutely 
%			continuous with 
%			respect to $ Q $, which requires that for every subset $ A $ of $ 
%			\Omega $, $ 
%			Q(A)=0 $ imply $ P(A)=0 $. 
\begin{itemize}	
		\item	\structure{\textbf{$ f 
		$-divergence}}  from $ P $ 
		to $ Q $~\cite{csiszar1964informationstheoretische}  
%			If $ P\ll Q $, 
			is defined by
			\begin{equation}\label{eq:definition_f_divergence}
			D_f(P\parallel Q) = \sum_{i\in \Omega} Q(i) f\left( 
			\frac{P(i)}{Q(i)} 
			\right) \,,
			\end{equation}
			where $ f $ is convex and $ f(1)=0 $.
%			 $ f:(0,\infty)\to 
%			\mathbb{R} $ be convex s.t.\ 
%			$ f(1)=0 
%			$. 
%			provided that the right-hand side exists,
%			where $ \frac{d P}{d Q} $ is the Radon-Nikodym derivative of $ P $ 
%			with 
%			respect to $ Q $.
			It is not symmetric in general: $ D_f(P\parallel 
			Q)\ne 
			D_f(Q\parallel P) $.
			
			\item \structure{\textbf{KL-Divergence}} $ \kl{P}{Q}= \sum_{i\in 
			\Omega} P(i) \ln\frac{P(i)}{Q(i)}  $ is the $ 
			f_{\mathrm{KL}} 
			$-divergence,
%			~\cite{cover2012elements}
			where
			 $ f_{\mathrm{KL}}(t) = t\ln t + (1-t) $.
			%If $ P\ll Q $, the \emph{Kullback-Leibler (KL) divergence} from $ 
			%P $ to $ Q $ 
			%is 
			%defined by 
%			We have
%			$
%			\kl{P}{Q} = \sum_{i\in \Omega} P(i) \ln\frac{P(i)}{Q(i)} 
%			$.
			
		\item	\structure{\textbf{Squared Hellinger 
		Distance}}
%	~\cite{daskalakis2017square}
	 $ H^2(P,Q) = 
		\frac{1}{2} \sum_{i\in \Omega} 
		(\sqrt{P(i)}-\sqrt{Q(i)})^2 $ is 
		the $ \hel 
		$-divergence, where
			 $ \hel(t)=\frac{1}{2}(\sqrt{t}-1)^2 $.
%			 We have
%			$
%			H^2(P,Q) = \frac{1}{2} \int_{\Omega} 
%			(\sqrt{dP}-\sqrt{dQ})^2$.	
		\item  \structure{\textbf{Triangular 
		Discrimination}}
%	~\citep{le2012asymptotic}
		$\Delta(P\parallel Q) = \sum_{i\in 
			\Omega} 
		\frac{(P(i)-Q(i))^2}{P(i)+Q(i)}$ is the $ \delta $-divergence, where
			 $ \delta(t) = \frac{(t-1)^2}{t+1} $.
%			 (also known as Vincze-Le Cam 
%			distance)
%			 If the sample 
%			space is 
%			finite, the triangular discrimination between $ P $ and $ Q $ is 
%			given by 
%			.
			
		\item \structure{\textbf{Jensen-Shannon (JS) Divergence}}
			 is a symmetrized version 
			of the 
			KL divergence. If 
%			$ P\ll Q $, $ Q\ll P $ and
			 $ M=(P+Q)/2 $, it 
			is defined by
			\begin{equation}\label{eq:jensen-shannon}
			\js{P}{Q} = \frac{1}{2} \kl{P}{M} + \frac{1}{2} \kl{Q}{M}\enspace.
			\end{equation}
\end{itemize}
	\end{block}
	\begin{block}{3. Mutual Information Loss}
		%		The mutual information loss arises naturally in many machine 
		%learning 
		%		tasks, 
		%		such as information-theoretic 
		%clustering~\citep{dhillon2003divisive} 
		%		and 
		%		categorical feature compression~\citep{bateni2019categorical}. 
		
		%	Suppose that two random variables $ X $ and $ C $ obeys a joint 
		%	distribution $ p(X,C) $. 
		Let $ X\in \cX $ be the feature value of a data item, $ C\in \cC $ be 
		its 
		label, and the joint distribution $ p(X,C) $ model a 
		dataset~\citep{bateni2019categorical}.
		
		%A labeled dataset can be modeled as a joint empirical distribution 
		%$\pr(X, 
		%C)$, where the random variable $X$ denotes the data item and $C$ 
		%denotes 
		%the label. 
		%	Let $\mathcal{X}$ and $\mathcal{C}$ denote the support of $X$ and 
		%$C$ (\ie, 
		%	the 
		%	universal set of all possible feature values and labels), 
		%respectively. 
		Consider clustering two feature values $ x,y $ into a new combined 
		value $ 
		z $:
%		This 
%		operation can be represented by the following map 
		%\[\pi_{x,y}: \mathcal{X}\to \mathcal{X}\setminus\{x,y\}\cup \{z\}\]
		%such that
		\[ 
		\pi_{x,y}: \mathcal{X}\to \mathcal{X}\setminus\{x,y\}\cup 
		\{z\}\quad\text{s.t.}\quad \pi_{x,y}(t) = \begin{cases}
		t, & t\in \cX\setminus \{x, y\}\,,\\
		z, & t = x,y\,.
		\end{cases}
		\]
		%	where $x$ and $y$ are the two feature values to be clustered and 
		%$z\notin 
		%	\cX$ 
		%	is the new combined feature value.
		To make the dataset after clustering
		%	 applying the 
		%	map 
		%	$\pi_{x,y}$ 
		preserve as much information of the original dataset as 
		possible, 
		%	one has to select two feature values $x$ and $y$ such that
		we want to minimize
		the mutual 
		information loss 
%		incurred by the clustering operation
		\[\mil(x,y) = I(X;C) - I(\pi_{x,y}(X);C)\,,\]
		%	is minimized,
		where $I(\cdot;\cdot)$ is the mutual 
		information~\cite{cover2012elements}.
		%	 between two random 
		%	variables~\cite{cover2012elements}. 
		$ \mil(x,y)=\mil(y,x)\ge 0 $ due to the data processing 
		inequality~\cite{cover2012elements}.
	\end{block}
		\end{column} %
 		\begin{column}{\sepwid}\end{column} %
		%
\begin{column}{\onecolwid} %
\vspace{-40pt}

\begin{block}{Generalized JS Divergence}
	\structure{\textbf{Generalized Jensen-Shannon divergence.}} If we let $P$ 
	and 
	$Q$ be the conditional distribution of $C$ given $X=x$ and $X=y$, 
	respectively, 
	such that $ P(c) = p(C=c| X=x) $ and $ Q(c)=p(C=c| X=y) $, the mutual 
	information loss can be re-written as
	\begin{equation}\label{eq:mil}
	\lambda \kl{P}{M_\lambda} + (1-\lambda) \kl{Q}{M_\lambda}\enspace,
	\end{equation}
	where  $\lambda=\frac{p(x)}{p(x)+p(y)}$ and the distribution $ 
	M_\lambda=\lambda P + (1-\lambda)Q $.
	% is a convex combination of $ P $ and $ Q $.
	%
	Note that \eqref{eq:mil} is a generalized version of 
	\eqref{eq:jensen-shannon}. 
	Therefore, we define the 
	%\emph{Mutual Information Loss (MIL) Divergence} (also known as the 
	\emph{generalized Jensen-Shannon (GJS) 
		divergence} between $P$ and 
	$Q$~\citep{lin1991divergence,ali1966general,dhillon2003divisive}
	by
	$
	\wjs{\lambda}{P}{Q} = \lambda \kl{P}{M_\lambda} + (1-\lambda) 
	\kl{Q}{M_\lambda}$,
	where $ \lambda\in [0,1] $ and $ M_\lambda=\lambda P + (1-\lambda)Q $. We 
	immediately have $ \wjs{1/2}{P}{Q} = \js{P}{Q} $, which indicates that the 
	JS divergence is indeed a special case of the GJS divergence when $ 
	\lambda=1/2 $. The GJS
	%generalized Jensen-Shannon
	divergence has 
	another equivalent definition
	$
	\wjs{\lambda}{P}{Q} = H(M_\lambda) - \lambda H(P) - 
	(1-\lambda) H(Q)$,
	where $H(\cdot)$ denotes the Shannon entropy~\cite{cover2012elements}. In 
	contrast to the MIL 
	divergence, the GJS $ \wjs{\lambda}{\cdot}{\cdot} $ is not symmetric in 
	general 
	as the weight $ \lambda\in [0,1] $ is fixed and not necessarily equal to $ 
	1/2 
	$. 
%	We will show in \cref{lem:wjs-as-f-divergence} that the GJS divergence 
%	is an 
%	$ f $-divergence.
\end{block}	
\vspace{3pt}

\begin{block}{4. Positive Definite Kernel and \kr Kernel}
	\structure{\textbf{Positive definite kernel~\citep{scholkopf2001kernel}}}
		Let $ \cX $ be a non-empty set. A symmetric, real-valued map $ 
		k:\cX\times\cX\to \bR $ is a positive definite kernel on $ \cX $ if for 
		all 
		positive integer $ n $, real numbers $ a_1,\dots,a_n\in \bR $, and $ 
		x,\dots,x_n\in \cX $, it holds that $
		\sum_{i=1}^n \sum_{j=1}^n a_i a_j k(x_i,x_j) \ge 0$.
	
	A kernel is said to be a \emph{\kr} kernel if it can be represented as 
	the difference of two positive definite kernels. 
	
	\structure{\textbf{\kr kernel~\citep{ong2004learning}}}
		Let $ \cX $ be a non-empty set. A symmetric, real-valued map $ 
		k:\cX\times\cX\to \bR $ is a \kr kernel on $ \cX $ if there exists two 
		positive definite kernels $ k_1 $ and $ k_2 $ on $ \cX $ such that $ 
		k(x,y)=k_1(x,y)-k_2(x,y) $ holds for all $ x,y\in \cX $.

\end{block}



\end{column} %
 		\begin{column}{\sepwid}\end{column} %
		%
\begin{column}{\onecolwid} %
	\vspace{-40pt}
	
	\begin{block}{5. LSH Schemes for $ f $-Divergences}
		We build LSH schemes for $ f $-divergences based on approximation via 
		another $ 
		f $-divergence if the latter admits an LSH family. If $ D_f $ and $ D_g 
		$ are 
		two divergences associated with convex functions $ f $ and $ g $ as 
		defined by 
		\eqref{eq:definition_f_divergence}, the approximation ratio of $ 
		D_f(P\parallel Q) $ to $ D_g(P \parallel Q) $ is determined by the 
		ratio of 
		the functions $ f $ and $ g $, as well as the ratio of $ P $ to $ Q $ 
		(to 
		be precise, $ \inf_{i\in \Omega} \frac{P(i)}{Q(i)} 
		$)~\citep{sason2016f}.
		
		%We also present $ f $-divergences with a known LSH family in 
		%\cref{tab:list_of_f_divergences}.
		
		% The $ \chi^2 $-divergence admits the $ \chi^2 
		%$-LSH family~\citep{gorisse2012locality}. The total variation between 
		%two 
		%probability measures is identical to the $ L^1 $ distance between the 
		%two 
		%vectors $ [P(i)]_{i\in \Omega} $ and $ [Q(i)]_{i\in \Omega} $. 
		
		%The $ L^2 $-LSH 
		% applies to  
		%the triangular discrimination after a transform of taking the square 
		%root.
		
		%\cref{thm:main} shows the guarantee of the LSH family build upon 
		%approximation 
		%via another $ f $-divergence.
		
		\structure{Proposition 1}[{Proof in \cref{app:main}}]\label{thm:main}
			Let $ \beta_0\in (0,1),L,U>0 $ and let $ f $ and $ g $ be two 
			convex 
			functions $ 
			(0,\infty)\to \mathbb{R} $ 
			that obey $ f(1)=0 $, $ g(1)=0 $, and $f(t), g(t)>0 $ for every $ 
			t\ne 1 
			$. 
			Let $ \mathcal{P} $ be a set of probability measures
			on a  finite sample space $ \Omega $ such that for every $ i\in 
			\Omega $ and $ 
			P, Q\in \mathcal{P} $, $
			0< \beta_0\le \frac{P(i)}{Q(i)} \le \beta_0^{-1}
			$.
			Assume that for every $ \beta\in 
			(\beta_0, 1)\cup (1,\beta_0^{-1}) $, it holds that
			$
			0<L\le \frac{f(\beta)}{g(\beta)} \le U < \infty$.
			%	 $ U=\sup_{\beta\in (\beta_0, 1)\cup (1,\beta_0^{-1})} 
			%f(\beta)/g(\beta) $ 
			%	 and $ L = \inf_{\beta\in (\beta_0, 1)\cup (1,\beta_0^{-1})} 
			%	 f(\beta)/g(\beta) $ are both finite and non-zero
			If $ \mathcal{H} $ forms 
			an $ (r_1, r_2, p_1,p_2) $-sensitive family for $ 
			g 
			$-divergence on $ \mathcal{P} $, then it is also an  
			$ (Lr_1, 
			Ur_2, p_1,p_2)  
			$-sensitive family for $ f $-divergence on $ \mathcal{P} $.
		
		
		\cref{thm:main} provides a general strategy of constructing LSH 
		families for $ 
		f $-divergences. The performance of such LSH families depends on the 
		tightness 
		of 
		the approximation. 
		In \cref{sub:weighted_js,sub:triangular_discrimination}, as instances 
		of the 
		general strategy, we derive concrete 
		results for the generalized Jensen-Shannon 
		divergence and triangular discrimination, respectively.
		
		\structure{Generalized Jensen-Shannon 
		Divergence}\label{sub:weighted_js}
		First, \cref{lem:wjs-as-f-divergence} shows that the GJS divergence is 
		indeed 
		an 
		instance of $ f $-divergence.
		\structure{lemma}
			Define 
			%	\begin{equation*}
			$	m_\lambda(t) = \lambda t \ln t - (\lambda t + 1 - 
			\lambda)\ln(\lambda t+1-\lambda)$.
			%	\end{equation*}
			For any $ \lambda\in[0,1] $, $ m_\lambda(t) $ is convex on $ 
			(0,\infty) $ 
			and $ m_\lambda(1)=0 $. Furthermore, $ m_\lambda $-divergence 
			yields the 
			GJS divergence with parameter $ \lambda $.
		
		We choose to approximate it via the squared Hellinger distance, which 
		plays a central role in the construction of the hash family with 
		desired 
		properties.
		
		The approximation guarantee is established in \cref{thm:approximation}. 
		We show 
		that the ratio of $ \wjs{\lambda}{P}{Q} $ to $ H^2(P,Q) $ is upper 
		bounded by the function $ U(\lambda) $ and lower bounded by the 
		function $ 
		L(\lambda) $. Furthermore, \cref{thm:approximation} shows that $ 
		U(\lambda)\le 
		1 $, which implies that the squared Hellinger distance is an upper 
		bound of the 
		GJS divergence.
		\begin{theorem}[{Proof in 
				\cref{app:approximation}}]\label{thm:approximation}
			We assume that the sample space $ \Omega $ is finite. Let $ P $ and 
			$ Q $ 
			be two different distributions on $ \Omega $.
			For every  $ 
			t>0 $ 
			and $ \lambda\in (0,1) $, we have
			\begin{equation*}
			%	\begin{split}
			L(\lambda)H^2(P, Q)\le 	\wjs{\lambda}{P}{Q}
			\le  U(\lambda)H^2(P, Q)\le 
			H^2(P, 
			Q) ,
			%	\end{split}
			\end{equation*}
			where $ L(\lambda) = 2 \min\{ \eta(\lambda), 
			\eta(1-\lambda) \} $, $ \eta(\lambda) = -\lambda \ln \lambda $ and 
			$ 
			U(\lambda) = 
			\frac{2\lambda(1-\lambda)}{1-2\lambda}\ln \frac{1-\lambda}{\lambda} 
			$.
		\end{theorem}
		
		We show \cref{thm:approximation} by showing a two-sided approximation 
		result regarding $ m_\lambda $ and $ \hel $. This result might be of 
		independent interest for other machine learning tasks, say, approximate 
		information-theoretic clustering~\cite{chaudhuri2008finding}.
		\begin{lemma}[Proof in \cref{app:bound}]\label{lem:bound}
			Define $ \kappa_\lambda(t) = \frac{m_\lambda(t)}{\hel(t)} $. For 
			every 
			$ 
			t>0 $ 
			and $ \lambda\in (0,1) $, we have
			$
			\kappa_\lambda(t) = \kappa_{1-\lambda}(1/t)
			$ and $
			\kappa_\lambda(t) \in [L(\lambda), U(\lambda) ]
			$.
			%	\begin{equation}\label{eq:property_of_kappa}
			%	\kappa_\lambda(t) = \kappa_{1-\lambda}(1/t)
			%	\end{equation}
			%	\begin{equation}\label{eq:kappa_bounds}
			%	\kappa_\lambda(t) \in [L(\lambda), U(\lambda) ].
			%	\end{equation}
		\end{lemma}
		
		We illustrate the upper and lower bound functions $ U(\lambda) $ and $ 
		L(\lambda) $ in \cref{sec:illu-bounds}. Recall that if $ \lambda=1/2 $, 
		the generalized Jensen-Shannon divergence reduces to the usual 
		Jensen-Shannon 
		divergence. \cref{thm:approximation} yields the approximation 
		guarantee
		$
		0.69< \ln 2 \le \frac{\js{P}{Q}}{H^2(P, Q)} \le 1$.
		
		%\subsection{Hash Family Definition}\label{sub:hash_family}
		
		If the common sample space $ \Omega $ with which the 
		two distributions $ P $ and $ Q $ are associated is finite, one can  
		identify $ P $ and Q with the $ |\Omega| $-dimensional vectors 
		$[P(i)]_{i\in 
			\Omega}$ and $ [Q(i)]_{i\in \Omega} $, respectively.
		In this case, 
		$
		H^2(P, Q) = \frac{1}{2} \| \sqrt{P}-\sqrt{Q} \|_2^2$,
		which is exactly half of the squared $ L^2 $ distance between the two 
		vectors $ 
		\sqrt{P} \triangleq [\sqrt{P(i)}]_{i\in \Omega} $ and $ \sqrt{Q} 
		\triangleq 
		[\sqrt{Q(i)}]_{i\in \Omega} $. Therefore, the squared Hellinger 
		distance can be 
		endowed with the $ L^2 $-LSH family~\citep{datar2004locality} applied 
		to the 
		square root of the vector. In light of this, 
		the locality-sensitive hash function that we propose for the 
		generalized Jensen-Shannon  divergence is 
		\begin{equation}\label{eq:hash_function}
		h_{\mathbf{a}, b}(P) = \left\lceil  \frac{\mathbf{a}\cdot \sqrt{P} +b 
		}{r} 
		\right\rceil,
		\end{equation}
		where $ \mathbf{a}\sim \mathcal{N}(0, I) $ is a $ |\Omega| 
		$-dimensional standard normal random vector, $ 
		\cdot $ denotes the inner product, 
		$ b $ is uniformly at random on $ [0, r] $, and $ r $ is a positive 
		real 
		number. 
		
		\structure{theorem}[{Proof in 
		\cref{sub:proof_wjs}}]\label{thm:lsh-family}
			Let $ c= \| 
			\sqrt{P}-\sqrt{Q} \|_2$ and $ f_2 $ be the probability density 
			function of the 
			absolute value of the standard normal distribution.	The hash 
			functions $ 
			\{h_{\mathbf{a},b}\}  $ defined in \eqref{eq:hash_function} form a 
			$ (R,c^2\frac{U(\lambda)}{L(\lambda)}R,p_1,p_2) 
			$-sensitive 
			family for the 
			generalized Jensen-Shannon divergence with parameter $ \lambda $, 
			where $ R>0 
			$, $ p_1 = p(1) $, $ p_2=p(c) $, and $
			p(u) = \int_0^r \frac{1}{u}f_2(t/u)(1-t/r)dt$.
		
		\structure{Triangular Discrimination}
		Recall that  triangular discrimination is the $ \delta $-divergence, 
		where $ 
		\delta(t) = \frac{(t-1)^2}{t+1} $.
		As shown in the proof of 
		\cref{thm:lsh_family_triangular_discrimination} 
		(\cref{sub:proof_triangular_discrimination}), 
		the function $ \delta $ 
		can be 
		approximated by the function $ \hel(t) $ that defines the squared 
		Hellinger 
		distance
		$
		1\le	\frac{\delta(t)}{\hel(t)} \le 2$.
		The squared Hellinger distance can be sketched via $ L^2 $-LSH after 
		taking the 
		square root, as exemplified in \cref{sub:weighted_js}.
		By \cref{thm:main}, the LSH family for the square Hellinger distance 
		also forms 
		an LSH family for the triangular discrimination.
		\cref{thm:lsh_family_triangular_discrimination} shows that the LSH 
		family 
		defined in  \eqref{eq:hash_function} form a 
		$ (R,2c^2R,p_1,p_2) 
		$-sensitive 
		family for 
		triangular discrimination.
		
		\structure{Theorem }
			Let $ c= \| 
			\sqrt{P}-\sqrt{Q} \|_2$ and $ f_2 $ be the probability density 
			function of 
			the 
			absolute value of the standard normal distribution.	The hash 
			functions $ 
			\{h_{\mathbf{a},b}\}  $ defined in \eqref{eq:hash_function} form a 
			$ (R,2c^2R,p_1,p_2) 
			$-sensitive 
			family for 
			triangular discrimination, where $ R>0 
			$, $ p_1 = p(1) $, $ p_2=p(c) $, and $
			p(u) = \int_0^r \frac{1}{u}f_2(t/u)(1-t/r)dt$.
		
	\end{block}
	

	
	\begin{block}{6. Bandit Submodular Set Maximization}
 	
	\end{block}
	
	\begin{block}{7. References}
		\bibliographystyle{plainnat}
		\bibliography{reference-list}
	\end{block}
	
\end{column} %
 
		
	\end{columns} %
	
\end{frame} %

\end{document}